{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kilometers/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kilometers/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kilometers/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# preprocessing documents\n",
    "def preprocess(document):\n",
    "    # 1. Lowercase Sentence\n",
    "    document = document.lower()\n",
    "\n",
    "    # 2. Sentence Splitter\n",
    "    bag_of_document = document.split(' ')\n",
    "\n",
    "    # 3. Punctuation Remove\n",
    "    for i, word in enumerate(bag_of_document):\n",
    "            bag_of_document[i] = word.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # 4. Stop Word Remove\n",
    "    #print(stopwords.words('english'))\n",
    "    bag_of_document = [word for word in bag_of_document if word not in stopwords.words('english')]\n",
    "\n",
    "    # 5. Lemmatize\n",
    "    for i, word in enumerate(bag_of_document):\n",
    "            bag_of_document[i] = lemmatizer.lemmatize(word)\n",
    "\n",
    "    bag_of_document = ' '.join(bag_of_document)\n",
    "\n",
    "    return bag_of_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "df.drop(['UserId', 'ProductId', 'Id', 'ProfileName','HelpfulnessDenominator', 'HelpfulnessNumerator', 'Time', 'Summary'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 568454/568454 [1:53:13<00:00, 83.68it/s]  \n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    df.at[idx, 'Text'] = preprocess(row['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>product arrived labeled jumbo salted peanutsth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>confection around century  light pillowy citru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>great taffy great price  wide assortment yummy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text  Rating\n",
       "0      5  bought several vitality canned dog food produc...       1\n",
       "1      1  product arrived labeled jumbo salted peanutsth...       0\n",
       "2      4  confection around century  light pillowy citru...       1\n",
       "3      2  looking secret ingredient robitussin believe f...       0\n",
       "4      5  great taffy great price  wide assortment yummy...       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Output.csv')\n",
    "\n",
    "df['Rating'] = df.Score.apply(lambda x: 1 if x in [3, 4, 5] else 0)\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 40000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "\n",
    "split = round(len(df)*0.7)\n",
    "\n",
    "train_text = df['Text'][:split]\n",
    "train_rating = df['Rating'][:split]\n",
    "\n",
    "test_text = df['Text'][split:]\n",
    "test_rating = df['Rating'][split:]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(train_text)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating='post')\n",
    "\n",
    "testing_sentences = tokenizer.texts_to_sequences(test_text)\n",
    "testing_padded = pad_sequences(testing_sentences, maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 120, 16)           640000    \n",
      "                                                                 \n",
      " global_average_pooling1d_3   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 102       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 640,109\n",
      "Trainable params: 640,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 13s 12ms/step - loss: 0.4339 - accuracy: 0.8509 - precision_2: 0.8544 - recall_2: 0.9950 - f1_score: 0.9214 - val_loss: 0.3396 - val_accuracy: 0.8608 - val_precision_2: 0.8609 - val_recall_2: 0.9994 - val_f1_score: 0.9241\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2601 - accuracy: 0.8904 - precision_2: 0.8969 - recall_2: 0.9850 - f1_score: 0.9214 - val_loss: 0.2261 - val_accuracy: 0.9101 - val_precision_2: 0.9241 - val_recall_2: 0.9754 - val_f1_score: 0.9241\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2135 - accuracy: 0.9145 - precision_2: 0.9281 - recall_2: 0.9755 - f1_score: 0.9214 - val_loss: 0.2119 - val_accuracy: 0.9177 - val_precision_2: 0.9342 - val_recall_2: 0.9727 - val_f1_score: 0.9241\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2006 - accuracy: 0.9210 - precision_2: 0.9350 - recall_2: 0.9753 - f1_score: 0.9214 - val_loss: 0.2054 - val_accuracy: 0.9209 - val_precision_2: 0.9374 - val_recall_2: 0.9729 - val_f1_score: 0.9241\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1922 - accuracy: 0.9249 - precision_2: 0.9389 - recall_2: 0.9756 - f1_score: 0.9214 - val_loss: 0.2010 - val_accuracy: 0.9222 - val_precision_2: 0.9376 - val_recall_2: 0.9742 - val_f1_score: 0.9241\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1856 - accuracy: 0.9279 - precision_2: 0.9415 - recall_2: 0.9762 - f1_score: 0.9214 - val_loss: 0.1984 - val_accuracy: 0.9231 - val_precision_2: 0.9382 - val_recall_2: 0.9747 - val_f1_score: 0.9241\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1802 - accuracy: 0.9304 - precision_2: 0.9437 - recall_2: 0.9767 - f1_score: 0.9214 - val_loss: 0.1965 - val_accuracy: 0.9243 - val_precision_2: 0.9405 - val_recall_2: 0.9734 - val_f1_score: 0.9241\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1755 - accuracy: 0.9324 - precision_2: 0.9457 - recall_2: 0.9770 - f1_score: 0.9214 - val_loss: 0.1950 - val_accuracy: 0.9250 - val_precision_2: 0.9413 - val_recall_2: 0.9733 - val_f1_score: 0.9241\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 11s 11ms/step - loss: 0.1712 - accuracy: 0.9342 - precision_2: 0.9472 - recall_2: 0.9774 - f1_score: 0.9214 - val_loss: 0.1945 - val_accuracy: 0.9254 - val_precision_2: 0.9393 - val_recall_2: 0.9763 - val_f1_score: 0.9241\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 13s 13ms/step - loss: 0.1672 - accuracy: 0.9359 - precision_2: 0.9489 - recall_2: 0.9777 - f1_score: 0.9214 - val_loss: 0.1936 - val_accuracy: 0.9264 - val_precision_2: 0.9437 - val_recall_2: 0.9724 - val_f1_score: 0.9241\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1634 - accuracy: 0.9376 - precision_2: 0.9504 - recall_2: 0.9780 - f1_score: 0.9214 - val_loss: 0.1929 - val_accuracy: 0.9270 - val_precision_2: 0.9430 - val_recall_2: 0.9739 - val_f1_score: 0.9241\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1599 - accuracy: 0.9391 - precision_2: 0.9519 - recall_2: 0.9781 - f1_score: 0.9214 - val_loss: 0.1930 - val_accuracy: 0.9265 - val_precision_2: 0.9426 - val_recall_2: 0.9737 - val_f1_score: 0.9241\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 0.1564 - accuracy: 0.9404 - precision_2: 0.9533 - recall_2: 0.9781 - f1_score: 0.9214 - val_loss: 0.1933 - val_accuracy: 0.9272 - val_precision_2: 0.9463 - val_recall_2: 0.9703 - val_f1_score: 0.9241\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1530 - accuracy: 0.9419 - precision_2: 0.9547 - recall_2: 0.9783 - f1_score: 0.9214 - val_loss: 0.1932 - val_accuracy: 0.9274 - val_precision_2: 0.9457 - val_recall_2: 0.9713 - val_f1_score: 0.9241\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1496 - accuracy: 0.9434 - precision_2: 0.9564 - recall_2: 0.9784 - f1_score: 0.9214 - val_loss: 0.1933 - val_accuracy: 0.9277 - val_precision_2: 0.9481 - val_recall_2: 0.9689 - val_f1_score: 0.9241\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1463 - accuracy: 0.9447 - precision_2: 0.9579 - recall_2: 0.9783 - f1_score: 0.9214 - val_loss: 0.1934 - val_accuracy: 0.9275 - val_precision_2: 0.9474 - val_recall_2: 0.9694 - val_f1_score: 0.9241\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1431 - accuracy: 0.9460 - precision_2: 0.9593 - recall_2: 0.9782 - f1_score: 0.9214 - val_loss: 0.1939 - val_accuracy: 0.9277 - val_precision_2: 0.9457 - val_recall_2: 0.9716 - val_f1_score: 0.9241\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1400 - accuracy: 0.9472 - precision_2: 0.9608 - recall_2: 0.9782 - f1_score: 0.9214 - val_loss: 0.1956 - val_accuracy: 0.9276 - val_precision_2: 0.9521 - val_recall_2: 0.9642 - val_f1_score: 0.9241\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1364 - accuracy: 0.9487 - precision_2: 0.9624 - recall_2: 0.9781 - f1_score: 0.9214 - val_loss: 0.1982 - val_accuracy: 0.9280 - val_precision_2: 0.9453 - val_recall_2: 0.9724 - val_f1_score: 0.9241\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1333 - accuracy: 0.9499 - precision_2: 0.9637 - recall_2: 0.9782 - f1_score: 0.9214 - val_loss: 0.1986 - val_accuracy: 0.9281 - val_precision_2: 0.9496 - val_recall_2: 0.9676 - val_f1_score: 0.9241\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "rnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.F1Score(num_classes=1)])\n",
    "\n",
    "#rnn_model.summary()\n",
    "\n",
    "num_epochs = 20\n",
    "history = rnn_model.fit(padded, train_rating, epochs=num_epochs, steps_per_epoch=1000,validation_data=(testing_padded, test_rating))\n",
    "\n",
    "rnn_model.save('rnn model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.69      0.73     24068\n",
      "           1       0.95      0.97      0.96    146468\n",
      "\n",
      "    accuracy                           0.93    170536\n",
      "   macro avg       0.86      0.83      0.84    170536\n",
      "weighted avg       0.93      0.93      0.93    170536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = rnn_model.predict(testing_padded)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred[y_pred>=0.5] = 1\n",
    "y_pred[y_pred<0.5] = 0\n",
    "\n",
    "print(classification_report(test_rating, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 227s 222ms/step - loss: 0.2337 - accuracy: 0.9074 - precision_4: 0.9253 - recall_4: 0.9699 - f1_score: 0.9214 - val_loss: 0.1919 - val_accuracy: 0.9235 - val_precision_4: 0.9483 - val_recall_4: 0.9634 - val_f1_score: 0.9241\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 225s 225ms/step - loss: 0.1724 - accuracy: 0.9321 - precision_4: 0.9517 - recall_4: 0.9698 - f1_score: 0.9214 - val_loss: 0.1842 - val_accuracy: 0.9282 - val_precision_4: 0.9539 - val_recall_4: 0.9630 - val_f1_score: 0.9241\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 225s 225ms/step - loss: 0.1525 - accuracy: 0.9409 - precision_4: 0.9584 - recall_4: 0.9730 - f1_score: 0.9214 - val_loss: 0.1823 - val_accuracy: 0.9291 - val_precision_4: 0.9562 - val_recall_4: 0.9614 - val_f1_score: 0.9241\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 218s 218ms/step - loss: 0.1343 - accuracy: 0.9483 - precision_4: 0.9639 - recall_4: 0.9760 - f1_score: 0.9214 - val_loss: 0.1843 - val_accuracy: 0.9315 - val_precision_4: 0.9532 - val_recall_4: 0.9677 - val_f1_score: 0.9241\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 222s 222ms/step - loss: 0.1184 - accuracy: 0.9548 - precision_4: 0.9685 - recall_4: 0.9789 - f1_score: 0.9214 - val_loss: 0.1887 - val_accuracy: 0.9297 - val_precision_4: 0.9436 - val_recall_4: 0.9765 - val_f1_score: 0.9241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: lstm model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fb5c7e60a60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fb5aef778b0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tfa.metrics.F1Score(num_classes=1)])\n",
    "\n",
    "num_epochs = 5\n",
    "history = lstm_model.fit(padded, train_rating, epochs=num_epochs, steps_per_epoch=1000,validation_data=(testing_padded, test_rating))\n",
    "\n",
    "lstm_model.save('lstm model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.65      0.72     24068\n",
      "           1       0.94      0.98      0.96    146468\n",
      "\n",
      "    accuracy                           0.93    170536\n",
      "   macro avg       0.88      0.81      0.84    170536\n",
      "weighted avg       0.93      0.93      0.93    170536\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lstm_model.predict(testing_padded)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred[y_pred>=0.5] = 1\n",
    "y_pred[y_pred<0.5] = 0\n",
    "\n",
    "print(classification_report(test_rating, y_pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
